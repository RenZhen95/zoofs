{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grey wolf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk3hUcNiJZRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i08m0QABrkTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "6b86344a-b8c7-4676-c1cf-3134ad4f1e0f"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "df=pd.DataFrame(data=data.data,columns=data.feature_names)\n",
        "df_target=data.target\n",
        "df=pd.DataFrame(data=data.data,columns=data.feature_names)\n",
        "df_target=data.target\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
              "0        17.99         10.38  ...          0.4601                  0.11890\n",
              "1        20.57         17.77  ...          0.2750                  0.08902\n",
              "2        19.69         21.25  ...          0.3613                  0.08758\n",
              "3        11.42         20.38  ...          0.6638                  0.17300\n",
              "4        20.29         14.34  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yCNFa4jrw0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df,df_target, test_size=0.2, random_state=42)\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf = KNeighborsClassifier(n_neighbors=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHi9yscD9zkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class silver_bhaediya:\n",
        "\n",
        "  def __init__(self,objective_function,n_iteration=50,population_size=50):\n",
        "    self.population_size = population_size\n",
        "    self.n_iteration=n_iteration\n",
        "    self.objective_function=objective_function\n",
        "\n",
        "  def initialize_population(self,X):\n",
        "        self.individuals =  np.random.randint(0,2,size=(self.population_size,X.shape[1]))\n",
        "\n",
        "\n",
        "  def evaluate_fitness(self,model,X_train,y_train,X_valid,y_valid):\n",
        "        scores =  []\n",
        "        for i,individual in enumerate(self.individuals):\n",
        "            chosen_features = [index for index in range(X_train.shape[1]) if individual[index]==1]\n",
        "            X_train_copy = X_train.iloc[:,chosen_features]\n",
        "            X_valid_copy = X_valid.iloc[:,chosen_features]\n",
        "            score = self.objective_function(model,X_train_copy,y_train,X_valid_copy,y_valid)\n",
        "            if score< self.best_score:\n",
        "              self.best_score=score\n",
        "              self.best_score_dimension=individual\n",
        "\n",
        "            scores.append(score)\n",
        "        self.fitness_scores = scores\n",
        " \n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    return 1/(1+np.exp( -10*(x-0.5) ))\n",
        "        \n",
        "  def fit(self,model,X_train,y_train,X_valid,y_valid,method=2):\n",
        "    self.initialize_population(X_train)\n",
        "    self.best_score=np.inf\n",
        "    self.best_score_dimension=np.ones(X_train.shape[1]) \n",
        "\n",
        "    self.alpha_wolf_dimension,self.alpha_wolf_fitness=np.ones(X_train.shape[1]),np.inf\n",
        "    self.beta_wolf_dimension,self.beta_wolf_fitness=np.ones(X_train.shape[1]),np.inf\n",
        "    self.delta_wolf_dimension,self.delta_wolf_fitness=np.ones(X_train.shape[1]),np.inf\n",
        "\n",
        "\n",
        "    for i in range(self.n_iteration):\n",
        "      a=2-2*((i+1)/self.n_iteration)\n",
        "      self.evaluate_fitness(model,X_train,y_train,X_valid,y_valid)\n",
        "\n",
        "      top_three_fitness_indexes=np.argsort(self.fitness_scores)[:3]\n",
        "\n",
        "      for fit,dim in zip( np.array(self.fitness_scores)[top_three_fitness_indexes],self.individuals[top_three_fitness_indexes] ):\n",
        "        \n",
        "        if fit<self.alpha_wolf_fitness:\n",
        "          self.delta_wolf_fitness=self.beta_wolf_fitness\n",
        "          self.beta_wolf_fitness=self.alpha_wolf_fitness\n",
        "          self.alpha_wolf_fitness=fit\n",
        "\n",
        "          self.delta_wolf_dimension=self.beta_wolf_dimension\n",
        "          self.beta_wolf_dimension=self.alpha_wolf_dimension\n",
        "          self.alpha_wolf_dimension=dim\n",
        "          continue\n",
        "\n",
        "        if ((fit>self.alpha_wolf_fitness)&(fit<self.beta_wolf_fitness)):\n",
        "          self.delta_wolf_fitness=self.beta_wolf_fitness\n",
        "          self.beta_wolf_fitness=fit\n",
        "\n",
        "          self.delta_wolf_dimension=self.beta_wolf_dimension\n",
        "          self.beta_wolf_dimension=dim\n",
        "          continue\n",
        "\n",
        "        if ((fit>self.beta_wolf_fitness)&(fit<self.delta_wolf_fitness)):\n",
        "          self.delta_wolf_fitness=fit\n",
        "          self.delta_wolf_dimension=dim\n",
        "          continue\n",
        "\n",
        "      if (method==1)|(method==2):\n",
        "        C1=2*np.random.random((self.population_size,X_train.shape[1]))\n",
        "        A1=2*a*np.random.random((self.population_size,X_train.shape[1]))-a\n",
        "        D_alpha=abs( C1*self.alpha_wolf_dimension - self.individuals )\n",
        "\n",
        "        C2=2*np.random.random((self.population_size,X_train.shape[1]))\n",
        "        A2=2*a*np.random.random((self.population_size,X_train.shape[1]))-a\n",
        "        D_beta=abs( C2*self.beta_wolf_dimension - self.individuals )\n",
        "\n",
        "        C3=2*np.random.random((self.population_size,X_train.shape[1]))\n",
        "        A3=2*a*np.random.random((self.population_size,X_train.shape[1]))-a\n",
        "        D_delta=abs( C3*self.delta_wolf_dimension - self.individuals )\n",
        "\n",
        "      if method==2:\n",
        "        X1=abs( self.alpha_wolf_dimension -A1*D_alpha )\n",
        "        X2=abs( self.beta_wolf_dimension - A2*D_beta  )\n",
        "        X3=abs( self.delta_wolf_dimension -A3*D_delta )\n",
        "        self.individuals=np.where(np.random.uniform(size=(self.population_size,X_train.shape[1]))<= self.sigmoid((X1+X2+X3)/3),1,0)\n",
        "\n",
        "      if method==1:\n",
        "        Y1=np.where(  (self.alpha_wolf_dimension+ np.where(self.sigmoid(A1*D_alpha)>np.random.uniform(size=(self.population_size,X_train.shape[1])),1,0) ) >=1,1,0)\n",
        "        Y2=np.where(  (self.beta_wolf_dimension+ np.where(self.sigmoid(A1*D_beta)>np.random.uniform(size=(self.population_size,X_train.shape[1])),1,0) ) >=1,1,0)\n",
        "        Y3=np.where(  (self.delta_wolf_dimension+ np.where(self.sigmoid(A1*D_delta)>np.random.uniform(size=(self.population_size,X_train.shape[1])),1,0) ) >=1,1,0)\n",
        "        r=np.random.uniform(size=(self.population_size,X_train.shape[1]))\n",
        "        self.individuals[r<(1/3)]=Y1[r<(1/3)]\n",
        "        self.individuals[(r>=(1/3))&(r<(2/3))]=Y2[(r>=(1/3))&(r<(2/3))]\n",
        "        self.individuals[r>=(2/3)]=Y3[r>=(2/3)]\n",
        "##############################################################################################################################################################\n",
        "                                                            #COMPETITIVE\n",
        "#############################################################################################################################################################\n",
        "      if method=='competitive':\n",
        "        indx=np.arange(0,self.population_size)\n",
        "        np.random.shuffle(indx)\n",
        "        grp=indx.reshape(int(self.population_size/2),2)\n",
        "        R_value=0.9-0.9*((i+1)/self.n_iteration)\n",
        "\n",
        "        winner_population=grp[np.arange(int(self.population_size/2)),np.array(self.fitness_scores)[grp].argmin(axis=1)]\n",
        "        looser_population=grp[np.arange(int(self.population_size/2)),np.array(self.fitness_scores)[grp].argmax(axis=1)]\n",
        "\n",
        "        C1=2*np.random.random((int(self.population_size/2),X_train.shape[1]))\n",
        "        A1=2*a*np.random.random((int(self.population_size/2),X_train.shape[1]))-a\n",
        "        D_alpha=abs( C1*self.alpha_wolf_dimension - (self.individuals[winner_population]-self.individuals[looser_population]) )  \n",
        "        X1=abs( self.alpha_wolf_dimension -A1*D_alpha )\n",
        "\n",
        "        C2=2*np.random.random((int(self.population_size/2),X_train.shape[1]))\n",
        "        A2=2*a*np.random.random((int(self.population_size/2),X_train.shape[1]))-a\n",
        "        D_beta=abs( C2*self.beta_wolf_dimension - (self.individuals[winner_population]-self.individuals[looser_population]) )  \n",
        "        X2=abs( self.beta_wolf_dimension -A2*D_beta )\n",
        "\n",
        "        C3=2*np.random.random((int(self.population_size/2),X_train.shape[1]))\n",
        "        A3=2*a*np.random.random((int(self.population_size/2),X_train.shape[1]))-a\n",
        "        D_delta=abs( C2*self.delta_wolf_dimension - (self.individuals[winner_population]-self.individuals[looser_population]) )  \n",
        "        X3=abs( self.delta_wolf_dimension -A3*D_delta )\n",
        "\n",
        "        self.individuals[looser_population]=np.where(np.random.uniform(size=(int(self.population_size/2),X_train.shape[1]))<= self.sigmoid((X1+X2+X3)/3),1,0)\n",
        "        \n",
        "        self.alpha_wolf_dimension=np.where( R_value>=np.random.uniform(size=(X_train.shape[1])),\n",
        "                                           np.random.randint(0,2,size=(X_train.shape[1])), self.alpha_wolf_dimension )\n",
        "        self.beta_wolf_dimension=np.where( R_value>=np.random.uniform(size=(X_train.shape[1])),\n",
        "                                           np.random.randint(0,2,size=(X_train.shape[1])), self.beta_wolf_dimension )\n",
        "        self.delta_wolf_dimension=np.where( R_value>=np.random.uniform(size=(X_train.shape[1])),\n",
        "                                           np.random.randint(0,2,size=(X_train.shape[1])), self.delta_wolf_dimension )\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      print('iteration ',i,' ',np.array(self.fitness_scores).min(),' |mean ',np.array(self.fitness_scores).mean(),' |best_score ',self.best_score)\n",
        "\n",
        "    return self.best_score_dimension\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKUH7tY0maH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "3ac67989-b22f-4ba3-dd2c-bf170c86e486"
      },
      "source": [
        "import scipy\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def score_model(model,X_train, y_train, X_valid, y_valid):\n",
        "    model.fit(X_train,y_train)\n",
        "    # return log_loss(y_valid,model.predict_proba(X_valid))\n",
        "    P = (model.predict(X_valid) != y_valid).mean()\n",
        "    alpha=0.01\n",
        "    # Compute for the objective function\n",
        "    j = alpha*(X_valid.shape[1]/30)+(1-alpha)*P\n",
        "    return j\n",
        "gaf=silver_bhaediya(score_model,50,20)\n",
        "res=gaf.fit(clf,X_train, y_train,X_test, y_test,'competitive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration  0   0.03071929824561403  |mean  0.05398245614035087  |best_score  0.03071929824561403\n",
            "iteration  1   0.03071929824561403  |mean  0.05361315789473684  |best_score  0.03071929824561403\n",
            "iteration  2   0.03071929824561403  |mean  0.055615789473684195  |best_score  0.03071929824561403\n",
            "iteration  3   0.03071929824561403  |mean  0.05573245614035087  |best_score  0.03071929824561403\n",
            "iteration  4   0.03071929824561403  |mean  0.06319736842105263  |best_score  0.03071929824561403\n",
            "iteration  5   0.03071929824561403  |mean  0.05920614035087718  |best_score  0.03071929824561403\n",
            "iteration  6   0.03071929824561403  |mean  0.06496842105263158  |best_score  0.03071929824561403\n",
            "iteration  7   0.03071929824561403  |mean  0.05725263157894737  |best_score  0.03071929824561403\n",
            "iteration  8   0.03071929824561403  |mean  0.052224561403508764  |best_score  0.03071929824561403\n",
            "iteration  9   0.03071929824561403  |mean  0.053059649122807015  |best_score  0.03071929824561403\n",
            "iteration  10   0.03071929824561403  |mean  0.05623245614035087  |best_score  0.03071929824561403\n",
            "iteration  11   0.03071929824561403  |mean  0.05466315789473684  |best_score  0.03071929824561403\n",
            "iteration  12   0.03071929824561403  |mean  0.048950877192982446  |best_score  0.03071929824561403\n",
            "iteration  13   0.03071929824561403  |mean  0.05955701754385965  |best_score  0.03071929824561403\n",
            "iteration  14   0.03071929824561403  |mean  0.05494736842105261  |best_score  0.03071929824561403\n",
            "iteration  15   0.03071929824561403  |mean  0.05192456140350876  |best_score  0.03071929824561403\n",
            "iteration  16   0.03071929824561403  |mean  0.05281052631578946  |best_score  0.03071929824561403\n",
            "iteration  17   0.03071929824561403  |mean  0.047766666666666666  |best_score  0.03071929824561403\n",
            "iteration  18   0.03071929824561403  |mean  0.050873684210526314  |best_score  0.03071929824561403\n",
            "iteration  19   0.03071929824561403  |mean  0.053412280701754376  |best_score  0.03071929824561403\n",
            "iteration  20   0.03071929824561403  |mean  0.056651754385964904  |best_score  0.03071929824561403\n",
            "iteration  21   0.03071929824561403  |mean  0.05120614035087719  |best_score  0.03071929824561403\n",
            "iteration  22   0.03071929824561403  |mean  0.056700877192982446  |best_score  0.03071929824561403\n",
            "iteration  23   0.03071929824561403  |mean  0.052325438596491226  |best_score  0.03071929824561403\n",
            "iteration  24   0.03071929824561403  |mean  0.052710526315789465  |best_score  0.03071929824561403\n",
            "iteration  25   0.03071929824561403  |mean  0.05194210526315789  |best_score  0.03071929824561403\n",
            "iteration  26   0.03071929824561403  |mean  0.05362982456140349  |best_score  0.03071929824561403\n",
            "iteration  27   0.03071929824561403  |mean  0.054080701754385965  |best_score  0.03071929824561403\n",
            "iteration  28   0.03071929824561403  |mean  0.0477517543859649  |best_score  0.03071929824561403\n",
            "iteration  29   0.03071929824561403  |mean  0.04588070175438595  |best_score  0.03071929824561403\n",
            "iteration  30   0.03071929824561403  |mean  0.04978859649122807  |best_score  0.03071929824561403\n",
            "iteration  31   0.03071929824561403  |mean  0.05474824561403509  |best_score  0.03071929824561403\n",
            "iteration  32   0.03071929824561403  |mean  0.054114035087719295  |best_score  0.03071929824561403\n",
            "iteration  33   0.03071929824561403  |mean  0.051809649122807  |best_score  0.03071929824561403\n",
            "iteration  34   0.03071929824561403  |mean  0.050707017543859645  |best_score  0.03071929824561403\n",
            "iteration  35   0.03071929824561403  |mean  0.051592105263157884  |best_score  0.03071929824561403\n",
            "iteration  36   0.03071929824561403  |mean  0.050339473684210524  |best_score  0.03071929824561403\n",
            "iteration  37   0.03071929824561403  |mean  0.05396315789473684  |best_score  0.03071929824561403\n",
            "iteration  38   0.03071929824561403  |mean  0.05476491228070175  |best_score  0.03071929824561403\n",
            "iteration  39   0.03071929824561403  |mean  0.05873947368421052  |best_score  0.03071929824561403\n",
            "iteration  40   0.03071929824561403  |mean  0.059023684210526305  |best_score  0.03071929824561403\n",
            "iteration  41   0.03071929824561403  |mean  0.05139210526315789  |best_score  0.03071929824561403\n",
            "iteration  42   0.03071929824561403  |mean  0.05683771929824562  |best_score  0.03071929824561403\n",
            "iteration  43   0.03071929824561403  |mean  0.05114298245614035  |best_score  0.03071929824561403\n",
            "iteration  44   0.021701754385964913  |mean  0.043643859649122804  |best_score  0.021701754385964913\n",
            "iteration  45   0.021701754385964913  |mean  0.048003508771929826  |best_score  0.021701754385964913\n",
            "iteration  46   0.021368421052631578  |mean  0.041139473684210524  |best_score  0.021368421052631578\n",
            "iteration  47   0.021368421052631578  |mean  0.03611403508771929  |best_score  0.021368421052631578\n",
            "iteration  48   0.021368421052631578  |mean  0.03232280701754386  |best_score  0.021368421052631578\n",
            "iteration  49   0.021368421052631578  |mean  0.03684912280701754  |best_score  0.021368421052631578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUp0u14PYX7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd5f5d8b-e17c-425b-be9b-3aed1a4864d4"
      },
      "source": [
        "np.arange(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4UzIHbFrtka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "2238df88-66b3-4001-a998-ba52701d2dac"
      },
      "source": [
        "import scipy\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def score_model(model,X_train, y_train, X_valid, y_valid):\n",
        "    model.fit(X_train,y_train)\n",
        "    # return log_loss(y_valid,model.predict_proba(X_valid))\n",
        "    P = (model.predict(X_valid) != y_valid).mean()\n",
        "    alpha=0.01\n",
        "    # Compute for the objective function\n",
        "    j = alpha*(X_valid.shape[1]/30)+(1-alpha)*P\n",
        "    return j\n",
        "gaf=silver_bhaediya(score_model,50,20)\n",
        "res=gaf.fit(clf,X_train, y_train,X_test, y_test,method=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration  0   0.02270175438596491  |mean  0.06583947368421053  |best_score  0.02270175438596491\n",
            "iteration  1   0.021701754385964913  |mean  0.05508245614035088  |best_score  0.021701754385964913\n",
            "iteration  2   0.021701754385964913  |mean  0.03980263157894737  |best_score  0.021701754385964913\n",
            "iteration  3   0.022035087719298244  |mean  0.029080701754385964  |best_score  0.021701754385964913\n",
            "iteration  4   0.021701754385964913  |mean  0.029632456140350878  |best_score  0.021701754385964913\n",
            "iteration  5   0.021701754385964913  |mean  0.022435087719298245  |best_score  0.021701754385964913\n",
            "iteration  6   0.021701754385964913  |mean  0.022501754385964908  |best_score  0.021701754385964913\n",
            "iteration  7   0.021701754385964913  |mean  0.022368421052631576  |best_score  0.021701754385964913\n",
            "iteration  8   0.021701754385964913  |mean  0.022335087719298242  |best_score  0.021701754385964913\n",
            "iteration  9   0.021701754385964913  |mean  0.022335087719298242  |best_score  0.021701754385964913\n",
            "iteration  10   0.021368421052631578  |mean  0.023220175438596488  |best_score  0.021368421052631578\n",
            "iteration  11   0.021368421052631578  |mean  0.022068421052631577  |best_score  0.021368421052631578\n",
            "iteration  12   0.021368421052631578  |mean  0.021835087719298242  |best_score  0.021368421052631578\n",
            "iteration  13   0.021368421052631578  |mean  0.021935087719298245  |best_score  0.021368421052631578\n",
            "iteration  14   0.021368421052631578  |mean  0.026227192982456137  |best_score  0.021368421052631578\n",
            "iteration  15   0.021368421052631578  |mean  0.021785087719298248  |best_score  0.021368421052631578\n",
            "iteration  16   0.021368421052631578  |mean  0.021751754385964914  |best_score  0.021368421052631578\n",
            "iteration  17   0.021368421052631578  |mean  0.021735087719298246  |best_score  0.021368421052631578\n",
            "iteration  18   0.021368421052631578  |mean  0.02527543859649123  |best_score  0.021368421052631578\n",
            "iteration  19   0.021368421052631578  |mean  0.02517543859649123  |best_score  0.021368421052631578\n",
            "iteration  20   0.021368421052631578  |mean  0.021801754385964912  |best_score  0.021368421052631578\n",
            "iteration  21   0.021701754385964913  |mean  0.025475438596491227  |best_score  0.021368421052631578\n",
            "iteration  22   0.021368421052631578  |mean  0.028782456140350875  |best_score  0.021368421052631578\n",
            "iteration  23   0.021368421052631578  |mean  0.02348859649122807  |best_score  0.021368421052631578\n",
            "iteration  24   0.021368421052631578  |mean  0.021851754385964907  |best_score  0.021368421052631578\n",
            "iteration  25   0.021368421052631578  |mean  0.02262017543859649  |best_score  0.021368421052631578\n",
            "iteration  26   0.021368421052631578  |mean  0.02185175438596491  |best_score  0.021368421052631578\n",
            "iteration  27   0.021368421052631578  |mean  0.022686842105263155  |best_score  0.021368421052631578\n",
            "iteration  28   0.021368421052631578  |mean  0.022620175438596492  |best_score  0.021368421052631578\n",
            "iteration  29   0.021368421052631578  |mean  0.02270350877192983  |best_score  0.021368421052631578\n",
            "iteration  30   0.021368421052631578  |mean  0.021785087719298248  |best_score  0.021368421052631578\n",
            "iteration  31   0.021368421052631578  |mean  0.026127192982456138  |best_score  0.021368421052631578\n",
            "iteration  32   0.021368421052631578  |mean  0.02186842105263158  |best_score  0.021368421052631578\n",
            "iteration  33   0.021368421052631578  |mean  0.021785087719298244  |best_score  0.021368421052631578\n",
            "iteration  34   0.021368421052631578  |mean  0.021718421052631578  |best_score  0.021368421052631578\n",
            "iteration  35   0.021368421052631578  |mean  0.021701754385964913  |best_score  0.021368421052631578\n",
            "iteration  36   0.021368421052631578  |mean  0.02176842105263158  |best_score  0.021368421052631578\n",
            "iteration  37   0.021368421052631578  |mean  0.02175175438596491  |best_score  0.021368421052631578\n",
            "iteration  38   0.021368421052631578  |mean  0.022703508771929826  |best_score  0.021368421052631578\n",
            "iteration  39   0.021368421052631578  |mean  0.02527543859649123  |best_score  0.021368421052631578\n",
            "iteration  40   0.021368421052631578  |mean  0.021651754385964908  |best_score  0.021368421052631578\n",
            "iteration  41   0.021368421052631578  |mean  0.026077192982456136  |best_score  0.021368421052631578\n",
            "iteration  42   0.021368421052631578  |mean  0.02176842105263158  |best_score  0.021368421052631578\n",
            "iteration  43   0.021368421052631578  |mean  0.02170175438596491  |best_score  0.021368421052631578\n",
            "iteration  44   0.021368421052631578  |mean  0.021685087719298245  |best_score  0.021368421052631578\n",
            "iteration  45   0.021368421052631578  |mean  0.021685087719298245  |best_score  0.021368421052631578\n",
            "iteration  46   0.021368421052631578  |mean  0.025058771929824558  |best_score  0.021368421052631578\n",
            "iteration  47   0.021368421052631578  |mean  0.022703508771929823  |best_score  0.021368421052631578\n",
            "iteration  48   0.021368421052631578  |mean  0.025242105263157892  |best_score  0.021368421052631578\n",
            "iteration  49   0.021368421052631578  |mean  0.025342105263157895  |best_score  0.021368421052631578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD0XWsUp6_gO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}