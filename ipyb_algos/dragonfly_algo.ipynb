{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dragonfly algo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "amw9_YSYlduu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "df=pd.DataFrame(data=data.data,columns=data.feature_names)\n",
        "df_target=data.target\n",
        "df=pd.DataFrame(data=data.data,columns=data.feature_names)\n",
        "df_target=data.target\n",
        "df.head()\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df,df_target, test_size=0.2, random_state=42)\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RODuGJEUlio8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class silver_bhaediya:\n",
        "\n",
        "  def __init__(self,objective_function,n_iteration=50,population_size=50):\n",
        "    self.population_size = population_size\n",
        "    self.n_iteration=n_iteration\n",
        "    self.objective_function=objective_function\n",
        "\n",
        "  def initialize_population(self,X):\n",
        "        self.individuals =  np.random.randint(0,2,size=(self.population_size,X.shape[1]))\n",
        "\n",
        "\n",
        "  def evaluate_fitness(self,model,X_train,y_train,X_valid,y_valid):\n",
        "        scores =  []\n",
        "        for i,individual in enumerate(self.individuals):\n",
        "            chosen_features = [index for index in range(X_train.shape[1]) if individual[index]==1]\n",
        "            X_train_copy = X_train.iloc[:,chosen_features]\n",
        "            X_valid_copy = X_valid.iloc[:,chosen_features]\n",
        "            score = self.objective_function(model,X_train_copy,y_train,X_valid_copy,y_valid)\n",
        "            if score< self.best_score:\n",
        "              self.best_score=score\n",
        "              self.best_score_dimension=individual\n",
        "\n",
        "            scores.append(score)\n",
        "        self.fitness_scores = scores\n",
        "\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    #return abs((2/math.pi)*np.arctan((math.pi/2)*x ))\n",
        "    return 1/(1+np.exp( -1*(x) ))\n",
        "        \n",
        "  def fit(self,model,X_train,y_train,X_valid,y_valid,method='linear'):\n",
        "    self.initialize_population(X_train)\n",
        "    self.best_score=np.inf\n",
        "    self.best_score_dimension=np.ones(X_train.shape[1]) \n",
        "    delta_x=np.zeros((self.population_size,X_train.shape[1]))\n",
        "    kbest=3\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(self.n_iteration):\n",
        "      self.evaluate_fitness(model,X_train,y_train,X_valid,y_valid)\n",
        "\n",
        "      if method=='linear':\n",
        "        s=0.2-(0.2*((i+1)/self.n_iteration))\n",
        "        e=0.1-(0.1*((i+1)/self.n_iteration))\n",
        "        a=0.0+(0.2*((i+1)/self.n_iteration))\n",
        "        c=0.0+(0.2*((i+1)/self.n_iteration))\n",
        "        f=0.0+(2*((i+1)/self.n_iteration))\n",
        "        w=0.9-(i+1)*(0.5)/(self.n_iteration)\n",
        "      \n",
        "      if method=='random':\n",
        "        if 2*(i+1)<=self.n_iteration:\n",
        "          pct=0.1-(0.2*(i+1)/self.n_iteration)\n",
        "        else:\n",
        "          pct=0\n",
        "        w=0.9-(i+1)*(0.5)/(self.n_iteration)\n",
        "        s=2*np.random.random()*pct\n",
        "        a=2*np.random.random()*pct\n",
        "        c=2*np.random.random()*pct\n",
        "        f=2*np.random.random()\n",
        "        e=pct\n",
        "      \n",
        "      if method=='quadraic':\n",
        "        w=0.9-(i+1)*(0.5)/(self.n_iteration)\n",
        "        s=0.2-(0.2*((i+1)/self.n_iteration))**2\n",
        "        e=0.1-(0.1*((i+1)/self.n_iteration))**2\n",
        "        a=0.0+(0.2*((i+1)/self.n_iteration))**2\n",
        "        c=0.0+(0.2*((i+1)/self.n_iteration))**2\n",
        "        f=0.0+(2*(i+1)/self.n_iteration)**2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      self.evaluate_fitness(model,X_train,y_train,X_valid,y_valid)\n",
        "      temp=individuals=self.individuals\n",
        "      temp_2=(( (temp.reshape(temp.shape[0],1,temp.shape[1])-temp.reshape(1,temp.shape[0],temp.shape[1])).reshape(temp.shape[0]**2,temp.shape[1])**2) )\n",
        "      temp_3=temp_2.reshape(temp.shape[0],temp.shape[0],temp.shape[1]).sum(axis=2)\n",
        "      zz=np.argsort(temp_3 ) \n",
        "      cc=[ list(iter1[iter1!=iter2]) for iter1,iter2 in zip(zz,np.arange(temp.shape[0])) ]\n",
        "\n",
        "      Si=-(np.repeat(individuals,kbest,axis=0).reshape(individuals.shape[0],kbest,individuals.shape[1])-individuals[np.array(cc)[:,:kbest]]).sum(axis=1)\n",
        "      Ai=delta_x[np.array(cc)[:,:kbest]].sum(axis=1)/kbest\n",
        "      Ci=individuals[np.array(cc)[:,:kbest]].sum(axis=1)/kbest-individuals\n",
        "      Fi=self.best_score_dimension-self.individuals\n",
        "      Ei=self.individuals+self.individuals[np.array(self.fitness_scores).argmax()]\n",
        "\n",
        "      delta_x=s*Si+a*Ai+c*Ci+f*Fi+e*Ei+w*delta_x\n",
        "      T=abs(delta_x)/np.sqrt(1+delta_x**2)\n",
        "\n",
        "      self.individuals=np.where(np.random.random((self.population_size,X_train.shape[1]))<T,np.logical_not(self.individuals).astype(int),individuals)\n",
        "    \n",
        "      print('iteration ',i,' ',np.array(self.fitness_scores).min(),' |mean ',np.array(self.fitness_scores).mean(),' |best_score ',self.best_score)\n",
        "\n",
        "    return self.best_score_dimension\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fcH3M6MG1_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "ec64e8a5-2e9a-4806-9867-f72fa40e9ed4"
      },
      "source": [
        "import scipy\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def score_model(model,X_train, y_train, X_valid, y_valid):\n",
        "    model.fit(X_train,y_train)\n",
        "    # return log_loss(y_valid,model.predict_proba(X_valid))\n",
        "    P = (model.predict(X_valid) != y_valid).mean()\n",
        "    alpha=0.01\n",
        "    # Compute for the objective function\n",
        "    j = alpha*(X_valid.shape[1]/30)+(1-alpha)*P\n",
        "    return j\n",
        "gaf=silver_bhaediya(score_model,50,20)\n",
        "res=gaf.fit(clf,X_train, y_train,X_test, y_test,'linear')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration  0   0.03773684210526316  |mean  0.0668421052631579  |best_score  0.03773684210526316\n",
            "iteration  1   0.03840350877192982  |mean  0.06193070175438596  |best_score  0.03773684210526316\n",
            "iteration  2   0.04007017543859649  |mean  0.06176228070175438  |best_score  0.03773684210526316\n",
            "iteration  3   0.04007017543859649  |mean  0.06403333333333334  |best_score  0.03773684210526316\n",
            "iteration  4   0.03873684210526315  |mean  0.05718684210526316  |best_score  0.03773684210526316\n",
            "iteration  5   0.0293859649122807  |mean  0.06187982456140352  |best_score  0.0293859649122807\n",
            "iteration  6   0.04007017543859649  |mean  0.06553684210526314  |best_score  0.0293859649122807\n",
            "iteration  7   0.04007017543859649  |mean  0.07099824561403509  |best_score  0.0293859649122807\n",
            "iteration  8   0.04007017543859649  |mean  0.06172982456140351  |best_score  0.0293859649122807\n",
            "iteration  9   0.03071929824561403  |mean  0.06445263157894736  |best_score  0.0293859649122807\n",
            "iteration  10   0.03840350877192982  |mean  0.0628  |best_score  0.0293859649122807\n",
            "iteration  11   0.03840350877192982  |mean  0.06071315789473684  |best_score  0.0293859649122807\n",
            "iteration  12   0.021701754385964913  |mean  0.05241315789473684  |best_score  0.021701754385964913\n",
            "iteration  13   0.02236842105263158  |mean  0.06599122807017543  |best_score  0.021701754385964913\n",
            "iteration  14   0.020701754385964912  |mean  0.054734210526315795  |best_score  0.020701754385964912\n",
            "iteration  15   0.04007017543859649  |mean  0.07150087719298244  |best_score  0.020701754385964912\n",
            "iteration  16   0.0293859649122807  |mean  0.05451666666666666  |best_score  0.020701754385964912\n",
            "iteration  17   0.0323859649122807  |mean  0.05997894736842104  |best_score  0.020701754385964912\n",
            "iteration  18   0.020368421052631577  |mean  0.059228070175438595  |best_score  0.020368421052631577\n",
            "iteration  19   0.03171929824561403  |mean  0.06856228070175438  |best_score  0.020368421052631577\n",
            "iteration  20   0.020701754385964912  |mean  0.06670877192982456  |best_score  0.020368421052631577\n",
            "iteration  21   0.03840350877192982  |mean  0.06056315789473683  |best_score  0.020368421052631577\n",
            "iteration  22   0.021368421052631578  |mean  0.06518947368421052  |best_score  0.020368421052631577\n",
            "iteration  23   0.03773684210526316  |mean  0.06717719298245614  |best_score  0.020368421052631577\n",
            "iteration  24   0.03807017543859649  |mean  0.06552368421052632  |best_score  0.020368421052631577\n",
            "iteration  25   0.03840350877192982  |mean  0.07482631578947367  |best_score  0.020368421052631577\n",
            "iteration  26   0.021368421052631578  |mean  0.06652543859649124  |best_score  0.020368421052631577\n",
            "iteration  27   0.03773684210526316  |mean  0.06165  |best_score  0.020368421052631577\n",
            "iteration  28   0.02236842105263158  |mean  0.07576052631578947  |best_score  0.020368421052631577\n",
            "iteration  29   0.039736842105263154  |mean  0.06408771929824561  |best_score  0.020368421052631577\n",
            "iteration  30   0.030052631578947366  |mean  0.06134824561403509  |best_score  0.020368421052631577\n",
            "iteration  31   0.039736842105263154  |mean  0.06784473684210526  |best_score  0.020368421052631577\n",
            "iteration  32   0.030052631578947366  |mean  0.06594035087719298  |best_score  0.020368421052631577\n",
            "iteration  33   0.022035087719298244  |mean  0.061599122807017535  |best_score  0.020368421052631577\n",
            "iteration  34   0.04040350877192982  |mean  0.07078333333333334  |best_score  0.020368421052631577\n",
            "iteration  35   0.03840350877192982  |mean  0.06181578947368423  |best_score  0.020368421052631577\n",
            "iteration  36   0.03940350877192982  |mean  0.0663078947368421  |best_score  0.020368421052631577\n",
            "iteration  37   0.03205263157894737  |mean  0.06365263157894736  |best_score  0.020368421052631577\n",
            "iteration  38   0.03205263157894737  |mean  0.06507192982456139  |best_score  0.020368421052631577\n",
            "iteration  39   0.03740350877192982  |mean  0.06730964912280703  |best_score  0.020368421052631577\n",
            "iteration  40   0.03271929824561403  |mean  0.06791052631578946  |best_score  0.020368421052631577\n",
            "iteration  41   0.03873684210526315  |mean  0.06896315789473684  |best_score  0.020368421052631577\n",
            "iteration  42   0.03873684210526315  |mean  0.0595938596491228  |best_score  0.020368421052631577\n",
            "iteration  43   0.03907017543859649  |mean  0.06730964912280703  |best_score  0.020368421052631577\n",
            "iteration  44   0.02971929824561403  |mean  0.0600280701754386  |best_score  0.020368421052631577\n",
            "iteration  45   0.03773684210526316  |mean  0.07039912280701753  |best_score  0.020368421052631577\n",
            "iteration  46   0.04842105263157894  |mean  0.06595701754385964  |best_score  0.020368421052631577\n",
            "iteration  47   0.039736842105263154  |mean  0.06819473684210527  |best_score  0.020368421052631577\n",
            "iteration  48   0.0303859649122807  |mean  0.06208245614035087  |best_score  0.020368421052631577\n",
            "iteration  49   0.03907017543859649  |mean  0.06876228070175439  |best_score  0.020368421052631577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJJAzVPXG7pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}